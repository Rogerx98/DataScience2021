{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Heity94/01_DataScience_2021/blob/main/TWSM/Class_notes_PH/PH_homework_class6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CHiyjUKpWoLI",
        "outputId": "6013ab4a-ed87-4c5f-a32f-9e8f0f33edc0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.19.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBYumu_VXq1X"
      },
      "source": [
        "https://huggingface.co/course/chapter1/3?fw=pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "l9Yi0TlYWqo3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "#possibly replace with e.g. util.pytorch_cos_sim from sentence_transformers\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "from transformers import pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wb1gSb1di8lP"
      },
      "outputs": [],
      "source": [
        "#this will download the default gpt2 model, around 1.2GB (or just 530MB??)\n",
        "#generator = pipeline(\"text-generation\")\n",
        "#generator(\"In this course, we will teach you how to\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQ0dYerEXoTX"
      },
      "source": [
        "Mask filling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "UXbt6fy3XpkR"
      },
      "outputs": [],
      "source": [
        "#unmasker = pipeline(\"fill-mask\")\n",
        "#unmasker(\"This course will teach you all about <mask> models.\", top_k=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJbI6XRUnrf5"
      },
      "source": [
        "### Tokenizer and models\n",
        "\n",
        "DistilBERT is a small, fast, cheap and light Transformer model trained by distilling BERT base. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERTâ€™s performances as measured on the GLUE language understanding benchmark.\n",
        "\n",
        "https://huggingface.co/docs/transformers/model_doc/distilbert\n",
        "\n",
        "The abstract from the paper is the following:\n",
        "\n",
        "> As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pretraining phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive biases learned by larger models during pretraining, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keNO8G67nqo_",
        "outputId": "510fdc19-5542-432f-c9ed-566f59dbd57d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': <tf.Tensor: shape=(2, 16), dtype=int32, numpy=\n",
            "array([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662,\n",
            "        12172,  2607,  2026,  2878,  2166,  1012,   102],\n",
            "       [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(2, 16), dtype=int32, numpy=\n",
            "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(2, 16), dtype=int32, numpy=\n",
            "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>}\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "#checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "checkpoint = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "raw_inputs = [\n",
        "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
        "    \"I hate this so much!\",\n",
        "]\n",
        "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"tf\")\n",
        "print(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjfyJDpd66fz",
        "outputId": "8731f664-def8-4549-976d-b5c88b69ae96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': <tf.Tensor: shape=(2, 7), dtype=int32, numpy=\n",
            "array([[  101, 18168,  2290,  1010,  2023,  3185,   102],\n",
            "       [  101,  1059, 24475,  2003,  2183,   102,     0]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(2, 7), dtype=int32, numpy=\n",
            "array([[0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(2, 7), dtype=int32, numpy=\n",
            "array([[1, 1, 1, 1, 1, 1, 1],\n",
            "       [1, 1, 1, 1, 1, 1, 0]], dtype=int32)>}\n",
            "['om', '##g']\n",
            "['lo', '##l']\n",
            "['bert']\n",
            "['interdisciplinary']\n",
            "['com', '##pre', '##hen', '##sibility']\n",
            "['transformers']\n"
          ]
        }
      ],
      "source": [
        "#What about slang akronyms ?\n",
        "IDs = tokenizer([\"OMG, this movie\", \"WTF is going\"], padding=True, truncation=True, return_tensors=\"tf\")\n",
        "print(IDs)\n",
        "print(tokenizer.tokenize(\"OMG\"))\n",
        "print(tokenizer.tokenize(\"LOL\"))\n",
        "print(tokenizer.tokenize(\"BERT\"))\n",
        "print(tokenizer.tokenize(\"Interdisciplinary\"))\n",
        "print(tokenizer.tokenize(\"comprehensibility\"))\n",
        "\n",
        "#notice that the full BERT tokenizer would split the following into sub words:\n",
        "print(tokenizer.tokenize(\"transformers\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iJHrYr1TxHS"
      },
      "source": [
        "Notice the identical tokens for identical words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGGDVojIofKd",
        "outputId": "95190777-e2ba-4db2-d948-302bd07b2166"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': <tf.Tensor: shape=(2, 13), dtype=int32, numpy=\n",
            "array([[ 101, 2129, 2515, 1996, 9812, 1997, 1996, 7861, 8270, 4667, 2272,\n",
            "        2055,  102],\n",
            "       [ 101, 2129, 2079, 2017, 2272, 2105, 1029,  102,    0,    0,    0,\n",
            "           0,    0]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(2, 13), dtype=int32, numpy=\n",
            "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(2, 13), dtype=int32, numpy=\n",
            "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]], dtype=int32)>}\n"
          ]
        }
      ],
      "source": [
        "raw_inputs = [\n",
        "    \"How does the dimension of the embedding come about\",\n",
        "    \"How do you come around?\",\n",
        "]\n",
        "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"tf\")\n",
        "print(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caOj_FxbDhJ1"
      },
      "source": [
        "Notice the different ways of calling the tokenizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cT1QWccSC-_v",
        "outputId": "5f15d21c-e32b-43bc-bf72-34d40dba2249"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[12170, 9737, 9441, 2024, 1999, 2152, 5157, 999]\n",
            "{'input_ids': <tf.Tensor: shape=(1, 10), dtype=int32, numpy=\n",
            "array([[  101, 12170,  9737,  9441,  2024,  1999,  2152,  5157,   999,\n",
            "          102]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(1, 10), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 10), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}\n"
          ]
        }
      ],
      "source": [
        "s = \"BIPM alumni are in high demand!\"\n",
        "tokens = tokenizer.tokenize(s)\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(ids)\n",
        "\n",
        "ids2 = tokenizer(s, padding=True, truncation=True, return_tensors=\"tf\")\n",
        "print(ids2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qs473no2CxYd"
      },
      "source": [
        "#### Decoding tokens\n",
        "\n",
        "Note that the decode method not only converts the indices back to tokens, but also groups together the tokens that were part of the same words to produce a readable sentence. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewfFlrv9DqaN",
        "outputId": "dc9bd1eb-223e-4d7a-b69b-c469002f00a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bipm alumni are in high demand!\n"
          ]
        }
      ],
      "source": [
        "decoded_string = tokenizer.decode(ids)\n",
        "print(decoded_string)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meBK0rgMCuvM"
      },
      "source": [
        "### Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfwDS3pGrBWF",
        "outputId": "f4ea424a-1341-400e-dd16-a71b42e442b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "from transformers import TFAutoModel\n",
        "\n",
        "#checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "checkpoint = \"bert-base-uncased\"\n",
        "model = TFAutoModel.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8x067hc5v9dO",
        "outputId": "a4915469-682c-4786-f1dc-e3bf06d42c9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 13, 768)\n"
          ]
        }
      ],
      "source": [
        "#outputs = model(inputs)\n",
        "#print(outputs.last_hidden_state.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnzSMY9nVYo2"
      },
      "source": [
        "So the output is high-dimensional (embedding of dimension 768 for each individual word!)\n",
        "\n",
        "Let's find out if the embedding is really contextual, i.e. whether identical single words are different depending on the sentence:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYPbR3uSwCfU",
        "outputId": "83b1b004-cbf8-4d09-a9ce-500a728eb7a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "similarity:\n",
            "0.912581205368042\n"
          ]
        }
      ],
      "source": [
        "\n",
        "How_embed_1 = outputs.last_hidden_state[0,0,:]\n",
        "How_embed_2 = outputs.last_hidden_state[1,0,:]\n",
        "print(np.sum(How_embed_1==How_embed_2))#0\n",
        "#outputs.last_hidden_state[0:2,0,0:10]\n",
        "\n",
        "\n",
        "print(\"similarity:\")\n",
        "print( 1 - cosine(How_embed_1, How_embed_2) ) \n",
        "#most_similar=data_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UYvNTjElJhc"
      },
      "source": [
        "### Tasks:\n",
        "\n",
        "1. Embedding Similarity \n",
        "  * Write a function that takes as input two sentences, finds identical words, computes their contextual embeddings and prints the cosine similarity.\n",
        "  * Ask interesting questions (case, punctuation,...)\n",
        "  * Use longer words and find similarity between sub words\n",
        "\n",
        "2. Load the IMBD reviews and sample 500 positive and negative reviews. \n",
        "  * Extract the embedding vectors\n",
        "  * Fit a Naive Bayes classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1"
      ],
      "metadata": {
        "id": "GDOi0_KlPs5p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def similarity_common_words(sentence1, sentence2): \n",
        "  raw_inputs = [\n",
        "    sentence1,\n",
        "    sentence2,\n",
        "]\n",
        "  words_in_common = list(set(sentence1.lower().split()).intersection(sentence2.lower().split()))\n",
        "  inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"tf\")\n",
        "  outputs = model(inputs)\n",
        "  index_sentence1 = [sentence1.lower().split().index(i) for i in words_in_common]\n",
        "  index_sentence2 = [sentence2.lower().split().index(i) for i in words_in_common]\n",
        "  for i, word in enumerate(words_in_common):\n",
        "    How_embed_1 = outputs.last_hidden_state[0,index_sentence1[i],:]\n",
        "    How_embed_2 = outputs.last_hidden_state[1,index_sentence2[i],:]\n",
        "    print(\"Similarity:\", word)\n",
        "    print(1 - cosine(How_embed_1, How_embed_2)) "
      ],
      "metadata": {
        "id": "x20AmtqqhpEp"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similarity_common_words(\"work is live\", \"live is work\")"
      ],
      "metadata": {
        "id": "ijz33mCahyhS",
        "outputId": "0f46acca-c1c1-449a-ffc9-588b54f27d29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity: live\n",
            "0.22911497950553894\n",
            "Similarity: is\n",
            "0.7664703130722046\n",
            "Similarity: work\n",
            "0.10128674656152725\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "similarity_common_words(\"I do not care\", \"I do care\")"
      ],
      "metadata": {
        "id": "kIuHCqsXoQMZ",
        "outputId": "f1e98c6d-153f-46de-88fb-7d954d808fb6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity: i\n",
            "0.9537032842636108\n",
            "Similarity: do\n",
            "0.8553025722503662\n",
            "Similarity: care\n",
            "0.5524778962135315\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2\n",
        "Load the IMBD reviews and sample 500 positive and negative reviews.\n",
        "\n",
        "Extract the embedding vectors\n",
        "Fit a Naive Bayes classifier"
      ],
      "metadata": {
        "id": "YzcN5XM3Pyt5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "outputId": "9ed6356a-733c-4bb3-95a8-4b8fb123f130",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZf7WmX0i65g"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.19.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51Y3Z0eNi65h"
      },
      "source": [
        "https://huggingface.co/course/chapter1/3?fw=pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FlpsvsOSi65h"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "#possibly replace with e.g. util.pytorch_cos_sim from sentence_transformers\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "from transformers import pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtEkrS2DKKjo",
        "outputId": "22771ad2-5ffe-4f16-b4fc-b4e282ba609c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle \n",
        "IMDB_path = \"/content/drive/MyDrive/Colab_Notebooks/02_HWR/00_data/IMDB\"\n",
        "\n",
        "ReloadIMDB=False\n",
        "\n",
        "if ReloadIMDB:\n",
        "  from tensorflow.keras.datasets import imdb\n",
        "  (train_data, train_labels), (test_data, test_labels) = imdb.load_data(\n",
        "    num_words=10000)\n",
        "\n",
        "  word_index = imdb.get_word_index()\n",
        "  reverse_word_index = dict(\n",
        "    [(value, key) for (key, value) in word_index.items()])\n",
        "  \n",
        "  N=len(train_data)\n",
        "  decoded_reviews = [\"\" for x in range(N)]\n",
        "\n",
        "  for j in range(N):\n",
        "    decoded_reviews[j] = \" \".join(\n",
        "      [reverse_word_index.get(i - 3, \"?\") for i in train_data[j]])  \n",
        "    \n",
        "    N=len(test_data)\n",
        "  decoded_reviews_test = [\"\" for x in range(N)]\n",
        "\n",
        "  for j in range(N):\n",
        "    decoded_reviews_test[j] = \" \".join(\n",
        "      [reverse_word_index.get(i - 3, \"?\") for i in test_data[j]])\n",
        "    \n",
        "  pickle.dump(decoded_reviews_test, open(IMDB_path + \"/decoded_reviews_test.pkl\", \"wb\"))\n",
        "  pickle.dump(decoded_reviews, open(IMDB_path + \"/decoded_reviews_train.pkl\", \"wb\"))\n",
        "  pickle.dump(train_labels, open(IMDB_path + \"/train_labels.pkl\", \"wb\"))\n",
        "  pickle.dump(test_labels, open(IMDB_path + \"/test_labels.pkl\", \"wb\"))\n",
        "else:\n",
        "  decoded_reviews_test = pickle.load(open(IMDB_path + \"/decoded_reviews_test.pkl\", \"rb\"))\n",
        "  decoded_reviews = pickle.load(open(IMDB_path + \"/decoded_reviews_train.pkl\", \"rb\"))\n",
        "  train_labels = pickle.load(open(IMDB_path + \"/train_labels.pkl\", \"rb\"))\n",
        "  test_labels = pickle.load(open(IMDB_path + \"/test_labels.pkl\", \"rb\"))\n"
      ],
      "metadata": {
        "id": "QycQz7VVQddy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample reviews (500 pos & 500 neg reviews)\n",
        "import pandas as pd\n",
        "reviews_df = pd.DataFrame([decoded_reviews, train_labels]).T\n",
        "reviews_df.columns = [\"review\", \"label\"]"
      ],
      "metadata": {
        "id": "2WfsgFoIYYo0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_reviews = reviews_df[reviews_df[\"label\"]==1].sample(50)\n",
        "neg_reviews = reviews_df[reviews_df[\"label\"]==0].sample(50)\n",
        "\n",
        "sample_reviews = pd.concat([pos_reviews, neg_reviews], axis=0)"
      ],
      "metadata": {
        "id": "9jdCVX9yY0Ek"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_reviews[\"label\"].value_counts()"
      ],
      "metadata": {
        "id": "_2cO1AilZXx8",
        "outputId": "365f6a3a-7f60-4771-dd79-dd827e4a02b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    50\n",
              "0    50\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, TFBertModel\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
        "#text = \"Replace me by any text you'd like.\"\n",
        "encoded_input = tokenizer(sample_reviews.review.tolist(), padding=True, truncation=True, return_tensors='tf')\n",
        "output = model(encoded_input)"
      ],
      "metadata": {
        "id": "trTuWXfSidwy",
        "outputId": "29ed8a0b-72bf-413a-e2ea-d96ac4fe0593",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output.last_hidden_state.shape"
      ],
      "metadata": {
        "id": "Yh0a5ZRTjDNX",
        "outputId": "206dfe4e-12fd-4f84-d2e6-a53fc543ff58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([100, 512, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB"
      ],
      "metadata": {
        "id": "pqBYF-U5lHgt"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.ops.numpy_ops import np_config\n",
        "np_config.enable_numpy_behavior()\n",
        "output.last_hidden_state.reshape(100,512*768)"
      ],
      "metadata": {
        "id": "kZ0NjovXmQFy",
        "outputId": "e3209495-5542-4368-8768-b6d8bee5152b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(100, 393216), dtype=float32, numpy=\n",
              "array([[-0.4639352 , -0.34315842,  0.3890403 , ..., -0.2493155 ,\n",
              "        -0.45264718, -0.2762106 ],\n",
              "       [-0.03637165,  0.00357627,  0.36077356, ...,  0.08964637,\n",
              "         0.44046456, -0.09319736],\n",
              "       [-0.08678686, -0.31876284,  0.18888038, ...,  0.15907845,\n",
              "         0.09382807, -0.28075856],\n",
              "       ...,\n",
              "       [-0.09061913,  0.02977815,  0.18268013, ..., -0.07719649,\n",
              "         0.16959175, -0.404427  ],\n",
              "       [-0.10033323,  0.22673324,  0.02108204, ..., -0.17467722,\n",
              "        -0.4322721 , -0.38697416],\n",
              "       [-0.38555938, -0.52579784,  0.61203086, ..., -0.20721212,\n",
              "        -0.14508834, -0.2929266 ]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "X_train = output.last_hidden_state.reshape(100,512*768)\n",
        "X_train_scaled = scaler.fit_transform(X_train)"
      ],
      "metadata": {
        "id": "iKL_x0VDmo3V"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = sample_reviews[\"label\"].tolist()"
      ],
      "metadata": {
        "id": "2G7U40IMnUU1"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = MultinomialNB()\n",
        "clf.fit(X_train_scaled, y_train)"
      ],
      "metadata": {
        "id": "TyOzNg6cl1vj",
        "outputId": "312768e3-530e-421d-9cc8-b186e28dbbb4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB()"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "# Predict the Test set results, determine accuracy\n",
        "#y_pred_s = clf.predict(X_train_scaled)\n",
        "print('Train accuracy: ', clf.score(X_train_scaled, y_train))\n"
      ],
      "metadata": {
        "id": "q7lzUmGIl7rF",
        "outputId": "d1cf525f-6417-4fb3-cf90-3c964d4eb5d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train accuracy:  0.79\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the confusion matrix\n",
        "print(classification_report(y_test, y_pred_s))"
      ],
      "metadata": {
        "id": "RscfKCP5l70E"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "class6.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}